{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOBSTER Data Engineering + EDA + Feature/Label Pipeline\n",
        "\n",
        "This notebook discovers LOBSTER message/orderbook pairs, validates alignment,\n",
        "computes microstructure features and labels, runs EDA, and writes artifacts\n",
        "to `results/`. It is parameterized via environment variables and `config.yaml`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterator, List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    num_levels: int = 10\n",
        "    topk: int = 5\n",
        "    horizons_events: List[int] | None = None\n",
        "    export_format: str = \"parquet\"\n",
        "    chunk_rows: int = 500000\n",
        "    output_dir: str = \"results\"\n",
        "    plotting: bool = True\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path: Path) -> \"Config\":\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Config not found: {path}\")\n",
        "        with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "            data = yaml.safe_load(handle) or {}\n",
        "        horizons = data.get(\"horizons_events\", [1, 5, 10])\n",
        "        return Config(\n",
        "            num_levels=int(data.get(\"num_levels\", 10)),\n",
        "            topk=int(data.get(\"topk\", 5)),\n",
        "            horizons_events=[int(h) for h in horizons],\n",
        "            export_format=str(data.get(\"export_format\", \"parquet\")),\n",
        "            chunk_rows=int(data.get(\"chunk_rows\", 500000)),\n",
        "            output_dir=str(data.get(\"output_dir\", \"results\")),\n",
        "            plotting=bool(data.get(\"plotting\", True)),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAIR = Tuple[Path, Path, str, str]\n",
        "MESSAGE_RE = re.compile(r\"(?P<ticker>[A-Za-z]+)_(?P<date>\\d{4}-\\d{2}-\\d{2}).*message.*\\.csv$\")\n",
        "ORDERBOOK_RE = re.compile(r\"(?P<ticker>[A-Za-z]+)_(?P<date>\\d{4}-\\d{2}-\\d{2}).*orderbook.*\\.csv$\")\n",
        "\n",
        "def _infer_from_filename(path: Path) -> Tuple[str, str] | None:\n",
        "    match = MESSAGE_RE.match(path.name) or ORDERBOOK_RE.match(path.name)\n",
        "    if not match:\n",
        "        return None\n",
        "    return match.group(\"ticker\"), match.group(\"date\")\n",
        "\n",
        "def _pair_from_directory(directory: Path) -> Tuple[Path, Path] | None:\n",
        "    message = directory / \"message.csv\"\n",
        "    orderbook = directory / \"orderbook.csv\"\n",
        "    if message.exists() and orderbook.exists():\n",
        "        return message, orderbook\n",
        "    return None\n",
        "\n",
        "def discover_pairs(data_root: Path, repo_root: Path) -> List[PAIR]:\n",
        "    pairs: Dict[Tuple[str, str], Tuple[Path, Path]] = {}\n",
        "    if data_root.exists():\n",
        "        for subdir in data_root.rglob(\"*\"):\n",
        "            if not subdir.is_dir():\n",
        "                continue\n",
        "            found = _pair_from_directory(subdir)\n",
        "            if found:\n",
        "                ticker = subdir.parent.name\n",
        "                date = subdir.name\n",
        "                pairs[(ticker, date)] = found\n",
        "\n",
        "    if not pairs:\n",
        "        for path in repo_root.rglob(\"*.csv\"):\n",
        "            if \"message\" in path.name:\n",
        "                inferred = _infer_from_filename(path)\n",
        "                if not inferred:\n",
        "                    continue\n",
        "                ticker, date = inferred\n",
        "                orderbook_candidate = path.with_name(path.name.replace(\"message\", \"orderbook\"))\n",
        "                if orderbook_candidate.exists():\n",
        "                    pairs[(ticker, date)] = (path, orderbook_candidate)\n",
        "\n",
        "    return [\n",
        "        (message, orderbook, ticker, date)\n",
        "        for (ticker, date), (message, orderbook) in sorted(pairs.items())\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MESSAGE_COLUMNS = [\"time_sec\", \"event_type\", \"order_id\", \"size\", \"price\", \"direction\"]\n",
        "MESSAGE_DTYPES = {\n",
        "    \"time_sec\": \"float64\",\n",
        "    \"event_type\": \"int8\",\n",
        "    \"order_id\": \"int64\",\n",
        "    \"size\": \"int64\",\n",
        "    \"price\": \"int64\",\n",
        "    \"direction\": \"int8\",\n",
        "}\n",
        "\n",
        "def _orderbook_columns(num_levels: int) -> list[str]:\n",
        "    cols = []\n",
        "    for level in range(1, num_levels + 1):\n",
        "        cols.extend([\n",
        "            f\"ask_price_{level}\",\n",
        "            f\"ask_size_{level}\",\n",
        "            f\"bid_price_{level}\",\n",
        "            f\"bid_size_{level}\",\n",
        "        ])\n",
        "    return cols\n",
        "\n",
        "def _orderbook_dtypes(num_levels: int) -> dict[str, str]:\n",
        "    dtypes: dict[str, str] = {}\n",
        "    for level in range(1, num_levels + 1):\n",
        "        dtypes[f\"ask_price_{level}\"] = \"int64\"\n",
        "        dtypes[f\"ask_size_{level}\"] = \"int64\"\n",
        "        dtypes[f\"bid_price_{level}\"] = \"int64\"\n",
        "        dtypes[f\"bid_size_{level}\"] = \"int64\"\n",
        "    return dtypes\n",
        "\n",
        "def load_full(message_path: Path, orderbook_path: Path, num_levels: int) -> pd.DataFrame:\n",
        "    message = pd.read_csv(\n",
        "        message_path,\n",
        "        header=None,\n",
        "        names=MESSAGE_COLUMNS,\n",
        "        dtype=MESSAGE_DTYPES,\n",
        "    )\n",
        "    orderbook = pd.read_csv(\n",
        "        orderbook_path,\n",
        "        header=None,\n",
        "        names=_orderbook_columns(num_levels),\n",
        "        dtype=_orderbook_dtypes(num_levels),\n",
        "    )\n",
        "    if len(message) != len(orderbook):\n",
        "        raise ValueError(\"Message and orderbook rows are misaligned.\")\n",
        "    df = pd.concat([message, orderbook], axis=1)\n",
        "    df[\"time_ns\"] = np.round(df[\"time_sec\"] * 1e9).astype(\"int64\")\n",
        "    return df\n",
        "\n",
        "def iter_load(\n",
        "    message_path: Path,\n",
        "    orderbook_path: Path,\n",
        "    num_levels: int,\n",
        "    chunk_rows: int,\n",
        ") -> Iterator[pd.DataFrame]:\n",
        "    message_iter = pd.read_csv(\n",
        "        message_path,\n",
        "        header=None,\n",
        "        names=MESSAGE_COLUMNS,\n",
        "        dtype=MESSAGE_DTYPES,\n",
        "        chunksize=chunk_rows,\n",
        "    )\n",
        "    orderbook_iter = pd.read_csv(\n",
        "        orderbook_path,\n",
        "        header=None,\n",
        "        names=_orderbook_columns(num_levels),\n",
        "        dtype=_orderbook_dtypes(num_levels),\n",
        "        chunksize=chunk_rows,\n",
        "    )\n",
        "    for message_chunk, orderbook_chunk in zip(message_iter, orderbook_iter):\n",
        "        if len(message_chunk) != len(orderbook_chunk):\n",
        "            raise ValueError(\"Chunk size mismatch between message and orderbook.\")\n",
        "        df = pd.concat([message_chunk, orderbook_chunk], axis=1)\n",
        "        df[\"time_ns\"] = np.round(df[\"time_sec\"] * 1e9).astype(\"int64\")\n",
        "        yield df\n",
        "\n",
        "def load_lobster(\n",
        "    message_path: Path,\n",
        "    orderbook_path: Path,\n",
        "    num_levels: int,\n",
        "    chunk_rows: int | None = None,\n",
        ") -> Tuple[pd.DataFrame | None, Iterator[pd.DataFrame] | None]:\n",
        "    if chunk_rows:\n",
        "        return None, iter_load(message_path, orderbook_path, num_levels, chunk_rows)\n",
        "    return load_full(message_path, orderbook_path, num_levels), None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DUMMY_BID = -9999999999\n",
        "DUMMY_ASK = 9999999999\n",
        "\n",
        "def _level_columns(prefix: str, num_levels: int) -> List[str]:\n",
        "    return [f\"{prefix}_{level}\" for level in range(1, num_levels + 1)]\n",
        "\n",
        "def add_halt_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"is_halt\"] = (df[\"event_type\"] == 7).astype(\"int8\")\n",
        "    return df\n",
        "\n",
        "def sanity_checks(df: pd.DataFrame, num_levels: int) -> Dict[str, int]:\n",
        "    best_bid = df[\"bid_price_1\"].replace(DUMMY_BID, np.nan)\n",
        "    best_ask = df[\"ask_price_1\"].replace(DUMMY_ASK, np.nan)\n",
        "    spread_ok = (best_bid < best_ask) | best_bid.isna() | best_ask.isna()\n",
        "\n",
        "    bid_sizes = df[_level_columns(\"bid_size\", num_levels)]\n",
        "    ask_sizes = df[_level_columns(\"ask_size\", num_levels)]\n",
        "    sizes_ok = (bid_sizes >= 0).all(axis=None) and (ask_sizes >= 0).all(axis=None)\n",
        "\n",
        "    bid_prices = df[_level_columns(\"bid_price\", num_levels)].replace(DUMMY_BID, np.nan)\n",
        "    ask_prices = df[_level_columns(\"ask_price\", num_levels)].replace(DUMMY_ASK, np.nan)\n",
        "\n",
        "    bid_monotonic = (\n",
        "        np.nan_to_num(bid_prices.values, nan=-np.inf)[:, :-1]\n",
        "        >= np.nan_to_num(bid_prices.values, nan=-np.inf)[:, 1:]\n",
        "    ).all()\n",
        "    ask_monotonic = (\n",
        "        np.nan_to_num(ask_prices.values, nan=np.inf)[:, :-1]\n",
        "        <= np.nan_to_num(ask_prices.values, nan=np.inf)[:, 1:]\n",
        "    ).all()\n",
        "\n",
        "    return {\n",
        "        \"rows\": int(len(df)),\n",
        "        \"spread_violations\": int((~spread_ok).sum()),\n",
        "        \"sizes_non_negative\": int(sizes_ok),\n",
        "        \"bid_monotonic\": int(bid_monotonic),\n",
        "        \"ask_monotonic\": int(ask_monotonic),\n",
        "        \"halt_rows\": int((df[\"event_type\"] == 7).sum()),\n",
        "    }\n",
        "\n",
        "def write_sanity(summary: Dict[str, int], out_path: Path) -> None:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "        json.dump(summary, handle, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _level_arrays(df: pd.DataFrame, prefix: str, num_levels: int) -> np.ndarray:\n",
        "    cols = [f\"{prefix}_{level}\" for level in range(1, num_levels + 1)]\n",
        "    return df[cols].to_numpy()\n",
        "\n",
        "def _mask_dummy_prices(prices: np.ndarray, side: str) -> np.ndarray:\n",
        "    if side == \"bid\":\n",
        "        return np.where(prices == DUMMY_BID, np.nan, prices)\n",
        "    return np.where(prices == DUMMY_ASK, np.nan, prices)\n",
        "\n",
        "def _topk_depth(sizes: np.ndarray, k: int) -> np.ndarray:\n",
        "    return np.nansum(sizes[:, :k], axis=1)\n",
        "\n",
        "def _rolling_event_rate(time_ns: np.ndarray, window_ns: int = int(1e9)) -> np.ndarray:\n",
        "    rates = np.zeros_like(time_ns, dtype=\"float64\")\n",
        "    left = 0\n",
        "    for idx, t in enumerate(time_ns):\n",
        "        while time_ns[left] < t - window_ns:\n",
        "            left += 1\n",
        "        window_count = idx - left + 1\n",
        "        rates[idx] = window_count / (window_ns / 1e9)\n",
        "    return rates\n",
        "\n",
        "def _rolling_sum(series: pd.Series, window: int) -> pd.Series:\n",
        "    return series.rolling(window=window, min_periods=1).sum()\n",
        "\n",
        "def _rolling_std(series: pd.Series, window: int) -> pd.Series:\n",
        "    return series.rolling(window=window, min_periods=2).std().fillna(0.0)\n",
        "\n",
        "def compute_features(df: pd.DataFrame, num_levels: int, topk: int) -> pd.DataFrame:\n",
        "    bid_prices = _mask_dummy_prices(_level_arrays(df, \"bid_price\", num_levels), \"bid\")\n",
        "    ask_prices = _mask_dummy_prices(_level_arrays(df, \"ask_price\", num_levels), \"ask\")\n",
        "    bid_sizes = _level_arrays(df, \"bid_size\", num_levels)\n",
        "    ask_sizes = _level_arrays(df, \"ask_size\", num_levels)\n",
        "\n",
        "    best_bid = bid_prices[:, 0]\n",
        "    best_ask = ask_prices[:, 0]\n",
        "    mid = (best_bid + best_ask) / 2.0\n",
        "    spread = best_ask - best_bid\n",
        "\n",
        "    level1_imb = (bid_sizes[:, 0] - ask_sizes[:, 0]) / (\n",
        "        bid_sizes[:, 0] + ask_sizes[:, 0]\n",
        "    )\n",
        "    bid_depth_k = _topk_depth(bid_sizes, topk)\n",
        "    ask_depth_k = _topk_depth(ask_sizes, topk)\n",
        "    topk_imb = (bid_depth_k - ask_depth_k) / (bid_depth_k + ask_depth_k)\n",
        "\n",
        "    microprice = (\n",
        "        best_bid * ask_sizes[:, 0] + best_ask * bid_sizes[:, 0]\n",
        "    ) / (bid_sizes[:, 0] + ask_sizes[:, 0])\n",
        "\n",
        "    time_ns = df[\"time_ns\"].to_numpy()\n",
        "    event_rate = _rolling_event_rate(time_ns)\n",
        "\n",
        "    sign = np.select(\n",
        "        [df[\"event_type\"].isin([1]), df[\"event_type\"].isin([2, 3, 4, 5])],\n",
        "        [1, -1],\n",
        "        default=0,\n",
        "    )\n",
        "    signed_flow = df[\"direction\"] * df[\"size\"] * sign\n",
        "    order_flow_imb = _rolling_sum(signed_flow, window=100)\n",
        "\n",
        "    mid_series = pd.Series(mid)\n",
        "    mid_ret = mid_series.pct_change().fillna(0.0)\n",
        "    realized_vol = _rolling_std(mid_ret, window=100)\n",
        "\n",
        "    features = df.copy()\n",
        "    features[\"best_bid\"] = best_bid\n",
        "    features[\"best_ask\"] = best_ask\n",
        "    features[\"mid\"] = mid\n",
        "    features[\"spread\"] = spread\n",
        "    features[\"level1_imbalance\"] = level1_imb\n",
        "    features[\"topk_imbalance\"] = topk_imb\n",
        "    features[\"depth_topk_bid\"] = bid_depth_k\n",
        "    features[\"depth_topk_ask\"] = ask_depth_k\n",
        "    features[\"depth_total_topk\"] = bid_depth_k + ask_depth_k\n",
        "    features[\"microprice\"] = microprice\n",
        "    features[\"event_rate\"] = event_rate\n",
        "    features[\"order_flow_imbalance\"] = order_flow_imb\n",
        "    features[\"realized_volatility\"] = realized_vol\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_labels(df: pd.DataFrame, horizons: List[int]) -> pd.DataFrame:\n",
        "    labeled = df.copy()\n",
        "    for horizon in horizons:\n",
        "        future_mid = labeled[\"mid\"].shift(-horizon)\n",
        "        raw_move = future_mid - labeled[\"mid\"]\n",
        "        threshold = np.maximum(labeled[\"spread\"] / 2.0, 1)\n",
        "        label = np.where(raw_move > threshold, 1, np.where(raw_move < -threshold, -1, 0))\n",
        "        labeled[f\"raw_move_H{horizon}\"] = raw_move\n",
        "        labeled[f\"label_H{horizon}\"] = label.astype(\"int8\")\n",
        "    return labeled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _save_plot(fig: plt.Figure, out_path: Path) -> None:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "def run_eda(df: pd.DataFrame, out_dir: Path, ticker: str, date: str) -> Dict[str, float]:\n",
        "    stats: Dict[str, float] = {}\n",
        "    time_sec = df[\"time_ns\"] / 1e9\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(time_sec, df[\"mid\"], linewidth=0.8)\n",
        "    ax.set_title(\"Midprice over time\")\n",
        "    ax.set_xlabel(\"Time (sec)\")\n",
        "    ax.set_ylabel(\"Midprice\")\n",
        "    _save_plot(fig, out_dir / f\"midprice_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(time_sec, df[\"spread\"], linewidth=0.8)\n",
        "    ax.set_title(\"Spread over time\")\n",
        "    ax.set_xlabel(\"Time (sec)\")\n",
        "    ax.set_ylabel(\"Spread\")\n",
        "    _save_plot(fig, out_dir / f\"spread_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(df[\"spread\"].dropna(), bins=50)\n",
        "    ax.set_title(\"Spread distribution\")\n",
        "    ax.set_xlabel(\"Spread\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    _save_plot(fig, out_dir / f\"spread_dist_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    event_counts = df[\"event_type\"].value_counts().sort_index()\n",
        "    ax.bar(event_counts.index.astype(str), event_counts.values)\n",
        "    ax.set_title(\"Event type distribution\")\n",
        "    ax.set_xlabel(\"Event type\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    _save_plot(fig, out_dir / f\"event_types_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    seconds = (df[\"time_ns\"] // 1_000_000_000).astype(int)\n",
        "    events_per_sec = seconds.value_counts().sort_index()\n",
        "    ax.plot(events_per_sec.index, events_per_sec.values)\n",
        "    ax.set_title(\"Events per second\")\n",
        "    ax.set_xlabel(\"Second\")\n",
        "    ax.set_ylabel(\"Events\")\n",
        "    _save_plot(fig, out_dir / f\"events_per_sec_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    bins = pd.qcut(df[\"topk_imbalance\"].fillna(0.0), q=10, duplicates=\"drop\")\n",
        "    grouped = df.groupby(bins)[\"raw_move_H1\"].mean()\n",
        "    ax.plot(grouped.index.astype(str), grouped.values, marker=\"o\")\n",
        "    ax.set_title(\"Imbalance vs future return\")\n",
        "    ax.set_xlabel(\"Imbalance decile\")\n",
        "    ax.set_ylabel(\"Mean future move (H1)\")\n",
        "    ax.tick_params(axis=\"x\", rotation=45)\n",
        "    _save_plot(fig, out_dir / f\"imbalance_future_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(df[\"realized_volatility\"], df[\"spread\"], s=3, alpha=0.5)\n",
        "    ax.set_title(\"Volatility vs spread\")\n",
        "    ax.set_xlabel(\"Realized volatility\")\n",
        "    ax.set_ylabel(\"Spread\")\n",
        "    _save_plot(fig, out_dir / f\"vol_vs_spread_{ticker}_{date}.png\")\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    is_cancel = df[\"event_type\"].isin([2, 3]).astype(int)\n",
        "    is_submit = (df[\"event_type\"] == 1).astype(int)\n",
        "    ratio = (\n",
        "        is_cancel.groupby(seconds).sum() / is_submit.groupby(seconds).sum().replace(0, np.nan)\n",
        "    )\n",
        "    ax.plot(ratio.index, ratio.values)\n",
        "    ax.set_title(\"Cancellation/submission ratio\")\n",
        "    ax.set_xlabel(\"Second\")\n",
        "    ax.set_ylabel(\"Cancel/Submit\")\n",
        "    _save_plot(fig, out_dir / f\"cancel_submit_{ticker}_{date}.png\")\n",
        "\n",
        "    spread_q = df[\"spread\"].quantile([0.33, 0.66])\n",
        "    vol_q = df[\"realized_volatility\"].quantile([0.33, 0.66])\n",
        "\n",
        "    spread_regime = pd.cut(\n",
        "        df[\"spread\"],\n",
        "        bins=[-np.inf, spread_q.iloc[0], spread_q.iloc[1], np.inf],\n",
        "        labels=[\"tight\", \"normal\", \"wide\"],\n",
        "    )\n",
        "    vol_regime = pd.cut(\n",
        "        df[\"realized_volatility\"],\n",
        "        bins=[-np.inf, vol_q.iloc[0], vol_q.iloc[1], np.inf],\n",
        "        labels=[\"low\", \"normal\", \"high\"],\n",
        "    )\n",
        "    regime_counts = df.groupby([spread_regime, vol_regime]).size().rename(\"count\")\n",
        "\n",
        "    stats[\"rows\"] = float(len(df))\n",
        "    stats[\"time_start\"] = float(time_sec.min())\n",
        "    stats[\"time_end\"] = float(time_sec.max())\n",
        "    stats[\"avg_spread\"] = float(df[\"spread\"].mean())\n",
        "    stats[\"avg_mid\"] = float(df[\"mid\"].mean())\n",
        "    stats[\"regime_counts\"] = regime_counts.to_dict()\n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_report(\n",
        "    out_path: Path,\n",
        "    ticker: str,\n",
        "    date: str,\n",
        "    stats: Dict[str, float],\n",
        "    sanity: Dict[str, int],\n",
        "    horizons: list[int],\n",
        ") -> None:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plot_prefix = f\"{ticker}_{date}\"\n",
        "    plot_files = [\n",
        "        f\"plots/midprice_{plot_prefix}.png\",\n",
        "        f\"plots/spread_{plot_prefix}.png\",\n",
        "        f\"plots/spread_dist_{plot_prefix}.png\",\n",
        "        f\"plots/event_types_{plot_prefix}.png\",\n",
        "        f\"plots/events_per_sec_{plot_prefix}.png\",\n",
        "        f\"plots/imbalance_future_{plot_prefix}.png\",\n",
        "        f\"plots/vol_vs_spread_{plot_prefix}.png\",\n",
        "        f\"plots/cancel_submit_{plot_prefix}.png\",\n",
        "    ]\n",
        "    regime_counts = stats.get(\"regime_counts\", {})\n",
        "    regime_lines = \"\\n\".join([f\"- {k}: {v}\" for k, v in regime_counts.items()])\n",
        "\n",
        "    content = f\"\"\"# EDA Report: {ticker} {date}\n",
        "\n",
        "## Dataset summary\n",
        "- Rows: {int(stats.get('rows', 0))}\n",
        "- Time range (sec): {stats.get('time_start', 0):.2f} to {stats.get('time_end', 0):.2f}\n",
        "- Average spread: {stats.get('avg_spread', 0):.2f}\n",
        "- Average mid: {stats.get('avg_mid', 0):.2f}\n",
        "\n",
        "## Sanity checks\n",
        "- Spread violations: {sanity.get('spread_violations', 0)}\n",
        "- Non-negative sizes: {sanity.get('sizes_non_negative', 0)}\n",
        "- Bid monotonic: {sanity.get('bid_monotonic', 0)}\n",
        "- Ask monotonic: {sanity.get('ask_monotonic', 0)}\n",
        "- Halt rows: {sanity.get('halt_rows', 0)}\n",
        "\n",
        "## Key EDA findings\n",
        "- Spread and midprice dynamics are shown in the plots below.\n",
        "- Event type distribution highlights liquidity dynamics (submissions vs cancellations).\n",
        "- Imbalance/future return plot provides a directional signal proxy at H=1 event horizon.\n",
        "- Volatility vs spread highlights liquidity regimes.\n",
        "\n",
        "## Regime segmentation counts\n",
        "{regime_lines or '- Not available'}\n",
        "\n",
        "## Plots\n",
        "\"\"\" + \"\\n\".join([f\"- {plot}\" for plot in plot_files]) + f\"\"\"\n",
        "\n",
        "## How this feeds the project\n",
        "1) Exported features include: best bid/ask, mid, spread, imbalance (L1/topK), depth, microprice,\n",
        "   event rate, order flow imbalance, and realized volatility.\n",
        "2) Recommended ML horizons: {', '.join(str(h) for h in horizons)} events, balancing fast reaction\n",
        "   (short horizon) and stability (longer horizon).\n",
        "3) Market making is typically most favorable in tight spread and low/normal volatility regimes.\n",
        "\"\"\"\n",
        "\n",
        "    out_path.write_text(content, encoding=\"utf-8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _export(df: pd.DataFrame, path: Path, export_format: str) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if export_format == \"parquet\":\n",
        "        df.to_parquet(path, index=False)\n",
        "    else:\n",
        "        df.to_csv(path, index=False)\n",
        "\n",
        "def _process_pair(message_path: Path, orderbook_path: Path, ticker: str, date: str, cfg: Config) -> None:\n",
        "    chunk_rows = cfg.chunk_rows if message_path.stat().st_size > 200 * 1024 * 1024 else None\n",
        "    full_df, iterator = load_lobster(message_path, orderbook_path, cfg.num_levels, chunk_rows)\n",
        "    if iterator is not None:\n",
        "        frames = [chunk for chunk in iterator]\n",
        "        df = pd.concat(frames, ignore_index=True)\n",
        "    else:\n",
        "        df = full_df\n",
        "    df = add_halt_flags(df)\n",
        "\n",
        "    sanity = sanity_checks(df, cfg.num_levels)\n",
        "    sanity_path = Path(cfg.output_dir) / \"sanity\" / f\"sanity_{ticker}_{date}.json\"\n",
        "    write_sanity(sanity, sanity_path)\n",
        "\n",
        "    features = compute_features(df, cfg.num_levels, cfg.topk)\n",
        "    labeled = generate_labels(features, cfg.horizons_events)\n",
        "\n",
        "    processed_path = Path(cfg.output_dir) / \"processed\" / f\"processed_{ticker}_{date}.{cfg.export_format}\"\n",
        "    _export(labeled, processed_path, cfg.export_format)\n",
        "\n",
        "    for horizon in cfg.horizons_events:\n",
        "        cols = [\n",
        "            \"time_ns\",\n",
        "            \"event_type\",\n",
        "            \"order_id\",\n",
        "            \"size\",\n",
        "            \"price\",\n",
        "            \"direction\",\n",
        "            \"best_bid\",\n",
        "            \"best_ask\",\n",
        "            \"mid\",\n",
        "            \"spread\",\n",
        "            \"is_halt\",\n",
        "            \"level1_imbalance\",\n",
        "            \"topk_imbalance\",\n",
        "            \"depth_topk_bid\",\n",
        "            \"depth_topk_ask\",\n",
        "            \"depth_total_topk\",\n",
        "            \"microprice\",\n",
        "            \"event_rate\",\n",
        "            \"order_flow_imbalance\",\n",
        "            \"realized_volatility\",\n",
        "            f\"raw_move_H{horizon}\",\n",
        "            f\"label_H{horizon}\",\n",
        "        ]\n",
        "        feature_df = labeled[cols]\n",
        "        feature_path = (\n",
        "            Path(cfg.output_dir)\n",
        "            / \"features\"\n",
        "            / f\"features_H{horizon}_{ticker}_{date}.{cfg.export_format}\"\n",
        "        )\n",
        "        _export(feature_df, feature_path, cfg.export_format)\n",
        "\n",
        "    if cfg.plotting:\n",
        "        plot_dir = Path(cfg.output_dir) / \"plots\"\n",
        "        stats = run_eda(labeled, plot_dir, ticker, date)\n",
        "    else:\n",
        "        stats = {}\n",
        "\n",
        "    report_path = Path(cfg.output_dir) / \"reports\" / f\"eda_report_{ticker}_{date}.md\"\n",
        "    write_report(report_path, ticker, date, stats, sanity, cfg.horizons_events)\n",
        "\n",
        "def run_pipeline() -> None:\n",
        "    data_root = Path(os.getenv(\"LOB_DATA_ROOT\", \"data/raw\"))\n",
        "    out_dir = os.getenv(\"LOB_OUT\", None)\n",
        "    ticker = os.getenv(\"LOB_TICKER\", None)\n",
        "    date = os.getenv(\"LOB_DATE\", None)\n",
        "    run_all = os.getenv(\"LOB_ALL\", \"0\") == \"1\"\n",
        "    export_format = os.getenv(\"LOB_FORMAT\", None)\n",
        "    no_plots = os.getenv(\"LOB_NO_PLOTS\", \"0\") == \"1\"\n",
        "\n",
        "    cfg = Config.load(Path(\"config.yaml\"))\n",
        "    if out_dir:\n",
        "        cfg.output_dir = out_dir\n",
        "    if export_format:\n",
        "        cfg.export_format = export_format\n",
        "    if no_plots:\n",
        "        cfg.plotting = False\n",
        "\n",
        "    pairs = discover_pairs(data_root, Path(\".\"))\n",
        "    if not pairs:\n",
        "        raise SystemExit(\"No dataset pairs found.\")\n",
        "\n",
        "    if run_all:\n",
        "        selected = pairs\n",
        "    else:\n",
        "        if not ticker or not date:\n",
        "            raise SystemExit(\"Provide LOB_TICKER and LOB_DATE or set LOB_ALL=1.\")\n",
        "        selected = [\n",
        "            pair\n",
        "            for pair in pairs\n",
        "            if pair[2].lower() == ticker.lower() and pair[3] == date\n",
        "        ]\n",
        "        if not selected:\n",
        "            raise SystemExit(\"Requested ticker/date not found in discovered pairs.\")\n",
        "\n",
        "    for message_path, orderbook_path, ticker, date in selected:\n",
        "        _process_pair(message_path, orderbook_path, ticker, date, cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_pipeline()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}